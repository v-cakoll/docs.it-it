---
title: Metriche di ML.NET
description: Informazioni sulle metriche usate per valutare le prestazioni di un modello ML.NET
ms.date: 04/29/2019
author: ''
ms.openlocfilehash: d76cab0b56085ebf2ee69f4d9d12c9685c3cb021
ms.sourcegitcommit: 4c10802ad003374641a2c2373b8a92e3c88babc8
ms.translationtype: HT
ms.contentlocale: it-IT
ms.lasthandoff: 05/08/2019
ms.locfileid: "65452704"
---
# <a name="model-evaluation-metrics-in-mlnet"></a><span data-ttu-id="77820-103">Metriche di valutazione dei modelli in ML.NET</span><span class="sxs-lookup"><span data-stu-id="77820-103">Model evaluation metrics in ML.NET</span></span>

## <a name="metrics-for-binary-classification"></a><span data-ttu-id="77820-104">Metriche per la classificazione binaria</span><span class="sxs-lookup"><span data-stu-id="77820-104">Metrics for Binary Classification</span></span>

| <span data-ttu-id="77820-105">Metrica</span><span class="sxs-lookup"><span data-stu-id="77820-105">Metrics</span></span>   |      <span data-ttu-id="77820-106">Descrizione</span><span class="sxs-lookup"><span data-stu-id="77820-106">Description</span></span>      |  <span data-ttu-id="77820-107">Obiettivo</span><span class="sxs-lookup"><span data-stu-id="77820-107">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="77820-108">**Accuracy**</span><span class="sxs-lookup"><span data-stu-id="77820-108">**Accuracy**</span></span> |  <span data-ttu-id="77820-109">[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) o accuratezza corrisponde alla percentuale di stime corrette con un set di dati di test.</span><span class="sxs-lookup"><span data-stu-id="77820-109">[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) is the proportion of correct predictions with a test data set.</span></span> <span data-ttu-id="77820-110">Equivale al rapporto tra il numero di stime corrette e il numero totale di campioni di input.</span><span class="sxs-lookup"><span data-stu-id="77820-110">It is the ratio of number of correct predictions to the total number of input samples.</span></span> <span data-ttu-id="77820-111">È una metrica valida solo nel caso di un numero simile di campioni appartenente a ogni classe.</span><span class="sxs-lookup"><span data-stu-id="77820-111">It works well only if there are similar number of samples belonging to each class.</span></span>| <span data-ttu-id="77820-112">**Quanto più vicino a 1,00, tanto meglio**.</span><span class="sxs-lookup"><span data-stu-id="77820-112">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="77820-113">Ma se il risultato è esattamente 1,00, significa che si è verificato un problema, in genere di tipo dispersione di dati (etichetta/destinazione), sovradattamento oppure uso di dati di training per il test.</span><span class="sxs-lookup"><span data-stu-id="77820-113">But exactly 1.00 indicates an issue (commonly: label/target leakage, over-fitting, or testing with training data).</span></span> <span data-ttu-id="77820-114">Se i dati di test sono squilibrati (per cui la maggior parte delle istanze appartengono a una delle classi), il set di dati è molto piccolo oppure i punteggi si avvicinano a 0,00 o 1,00, l'accuratezza non rileva in realtà l'efficacia di un classificatore ed è necessario controllare altre metriche.</span><span class="sxs-lookup"><span data-stu-id="77820-114">When the test data is unbalanced (where most of the instances belong to one of the classes), the dataset is very small, or scores approach 0.00 or 1.00, then accuracy doesn’t really capture the effectiveness of a classifier and you need to check additional metrics.</span></span> |
| <span data-ttu-id="77820-115">**AUC**</span><span class="sxs-lookup"><span data-stu-id="77820-115">**AUC**</span></span> |    <span data-ttu-id="77820-116">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) o *area sotto la curva*: corrisponde alla misurazione dell'area sotto la curva creata verificando la percentuale di veri positivi rispetto a quella di falsi positivi.</span><span class="sxs-lookup"><span data-stu-id="77820-116">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) or *Area under the curve*: This is measuring the area under the curve created by sweeping the true positive rate vs. the false positive rate.</span></span>  |   <span data-ttu-id="77820-117">**Quanto più vicino a 1,00, tanto meglio**.</span><span class="sxs-lookup"><span data-stu-id="77820-117">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="77820-118">Il valore dovrebbe essere maggiore di 0,50 perché un modello sia accettabile; un modello con AUC minore o uguale 0,50 è inutile.</span><span class="sxs-lookup"><span data-stu-id="77820-118">It should be greater than 0.50 for a model to be acceptable; a model with AUC of 0.50 or less is worthless.</span></span> |
| <span data-ttu-id="77820-119">**AUCPR**</span><span class="sxs-lookup"><span data-stu-id="77820-119">**AUCPR**</span></span> | <span data-ttu-id="77820-120">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) o *area sotto la curva di precisione-recupero*: si tratta di una misura utile del successo di una stima quando le classi sono molto sbilanciate (set di dati ampiamente distorti).</span><span class="sxs-lookup"><span data-stu-id="77820-120">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) or *Area under the curve of a Precision-Recall curve*: Useful measure of success of prediction when the classes are very imbalanced (highly skewed datasets).</span></span> |  <span data-ttu-id="77820-121">**Quanto più vicino a 1,00, tanto meglio**.</span><span class="sxs-lookup"><span data-stu-id="77820-121">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="77820-122">I punteggi elevati vicini a 1,00 mostrano che il classificatore restituisce risultati accurati (alta precisione), oltre a restituire una maggioranza di risultati tutti positivi (alto recupero).</span><span class="sxs-lookup"><span data-stu-id="77820-122">High scores close to 1.00 show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</span></span> |
| <span data-ttu-id="77820-123">**F1-score**</span><span class="sxs-lookup"><span data-stu-id="77820-123">**F1-score**</span></span> | <span data-ttu-id="77820-124">[F1 score](https://en.wikipedia.org/wiki/F1_score) anche detto *F-score bilanciato o F-measure*.</span><span class="sxs-lookup"><span data-stu-id="77820-124">[F1 score](https://en.wikipedia.org/wiki/F1_score) also known as *balanced F-score or F-measure*.</span></span> <span data-ttu-id="77820-125">Si tratta della media armonica di precisione e recupero.</span><span class="sxs-lookup"><span data-stu-id="77820-125">It's the harmonic mean of the precision and recall.</span></span> <span data-ttu-id="77820-126">La metrica F1 Score è utile se si vuole trovare un equilibrio tra precisione e recupero.</span><span class="sxs-lookup"><span data-stu-id="77820-126">F1 Score is helpful when you want to seek a balance between Precision and Recall.</span></span>| <span data-ttu-id="77820-127">**Quanto più vicino a 1,00, tanto meglio**.</span><span class="sxs-lookup"><span data-stu-id="77820-127">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="77820-128">F1-score raggiunge il valore ottimale con un punteggio 1,00 e il valore peggiore con 0,00.</span><span class="sxs-lookup"><span data-stu-id="77820-128">An F1 score reaches its best value at 1.00 and worst score at 0.00.</span></span> <span data-ttu-id="77820-129">Indica il grado di precisione del classificatore.</span><span class="sxs-lookup"><span data-stu-id="77820-129">It tells you how precise your classifier is.</span></span> |

<span data-ttu-id="77820-130">Per altre informazioni sulle metriche di classificazione binaria, vedere gli articoli seguenti:</span><span class="sxs-lookup"><span data-stu-id="77820-130">For further details on binary classification metrics read the following articles:</span></span>

- [<span data-ttu-id="77820-131">Accuracy, Precision, Recall or F1?</span><span class="sxs-lookup"><span data-stu-id="77820-131">Accuracy, Precision, Recall or F1?</span></span>](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)
- [<span data-ttu-id="77820-132">Classe BinaryClassificationMetrics</span><span class="sxs-lookup"><span data-stu-id="77820-132">Binary Classification Metrics class</span></span>](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.binaryclassificationmetrics?view=ml-dotnet)
- [<span data-ttu-id="77820-133">The Relationship Between Precision-Recall and ROC Curves</span><span class="sxs-lookup"><span data-stu-id="77820-133">The Relationship Between Precision-Recall and ROC Curves</span></span>](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)

## <a name="metrics-for-multi-class-classification"></a><span data-ttu-id="77820-134">Metriche per la classificazione multiclasse</span><span class="sxs-lookup"><span data-stu-id="77820-134">Metrics for Multi-class Classification</span></span>

| <span data-ttu-id="77820-135">Metrica</span><span class="sxs-lookup"><span data-stu-id="77820-135">Metrics</span></span>   |      <span data-ttu-id="77820-136">Descrizione</span><span class="sxs-lookup"><span data-stu-id="77820-136">Description</span></span>      |  <span data-ttu-id="77820-137">Obiettivo</span><span class="sxs-lookup"><span data-stu-id="77820-137">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="77820-138">**Micro-Accuracy**</span><span class="sxs-lookup"><span data-stu-id="77820-138">**Micro-Accuracy**</span></span> |  <span data-ttu-id="77820-139">L'[accuratezza micro-media](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.multiclassclassificationmetrics.microaccuracy?view=ml-dotnet) aggrega i contributi di tutte le classi per calcolare la metrica media.</span><span class="sxs-lookup"><span data-stu-id="77820-139">[Micro-average Accuracy](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.multiclassclassificationmetrics.microaccuracy?view=ml-dotnet) aggregates the contributions of all classes to compute the average metric.</span></span> <span data-ttu-id="77820-140">Corrisponde alla percentuale di istanze stimate correttamente.</span><span class="sxs-lookup"><span data-stu-id="77820-140">It is the fraction of instances predicted correctly.</span></span> <span data-ttu-id="77820-141">La micro-media non tiene conto dell'appartenenza a una classe.</span><span class="sxs-lookup"><span data-stu-id="77820-141">The micro-average does not take class membership into account.</span></span> <span data-ttu-id="77820-142">Essenzialmente, ogni coppia campione-classe contribuisce nello stesso modo alla metrica di accuratezza.</span><span class="sxs-lookup"><span data-stu-id="77820-142">Basically, every sample-class pair contributes equally to the accuracy metric.</span></span> | <span data-ttu-id="77820-143">**Quanto più vicino a 1,00, tanto meglio**.</span><span class="sxs-lookup"><span data-stu-id="77820-143">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="77820-144">In un'attività di classificazione multiclasse la micro-accuratezza è preferibile rispetto alla macro-accuratezza se si sospetta uno squilibrio di classi, ossia</span><span class="sxs-lookup"><span data-stu-id="77820-144">In a multi-class classification task, micro-accuracy is preferable over macro-accuracy if you suspect there might be class imbalance (i.e</span></span> <span data-ttu-id="77820-145">la presenza di molti più esempi di una classe rispetto ad altre.</span><span class="sxs-lookup"><span data-stu-id="77820-145">you may have many more examples of one class than of other classes).</span></span>|
| <span data-ttu-id="77820-146">**Macro-Accuracy**</span><span class="sxs-lookup"><span data-stu-id="77820-146">**Macro-Accuracy**</span></span> | <span data-ttu-id="77820-147">L'[accuratezza macro-media](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.multiclassclassificationmetrics.macroaccuracy?view=ml-dotnet) corrisponde all'accuratezza media a livello di classe.</span><span class="sxs-lookup"><span data-stu-id="77820-147">[Macro-average Accuracy](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.multiclassclassificationmetrics.macroaccuracy?view=ml-dotnet) is the average accuracy at the class level.</span></span> <span data-ttu-id="77820-148">Viene confrontata l'accuratezza per ogni classe e l'accuratezza macro-media è la media di queste accuratezze.</span><span class="sxs-lookup"><span data-stu-id="77820-148">The accuracy for each class is computed and the macro-accuracy is the average of these accuracies.</span></span> <span data-ttu-id="77820-149">Essenzialmente, ogni classe contribuisce nello stesso modo alla metrica di accuratezza.</span><span class="sxs-lookup"><span data-stu-id="77820-149">Basically, every class contributes equally to the accuracy metric.</span></span> <span data-ttu-id="77820-150">Alle classi di minoranza viene assegnato un peso uguale a quello delle classi più grandi.</span><span class="sxs-lookup"><span data-stu-id="77820-150">Minority classes are given equal weight as the larger classes.</span></span> <span data-ttu-id="77820-151">La metrica della macro-media assegna lo stesso peso a ogni classe, indipendentemente dal numero di istanze di tale classe contenute nel set di dati.</span><span class="sxs-lookup"><span data-stu-id="77820-151">The macro-average metric gives the same weight to each class, no matter how many instances from that class the dataset contains.</span></span> |  <span data-ttu-id="77820-152">**Quanto più vicino a 1,00, tanto meglio**.</span><span class="sxs-lookup"><span data-stu-id="77820-152">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="77820-153">Calcola la metrica in modo indipendente per ogni classe e quindi ne considera la media, di conseguenza tratta tutte le classi allo stesso modo</span><span class="sxs-lookup"><span data-stu-id="77820-153">It computes the metric independently for each class and then takes the average (hence treating all classes equally)</span></span> |
| <span data-ttu-id="77820-154">**Log-loss**</span><span class="sxs-lookup"><span data-stu-id="77820-154">**Log-loss**</span></span>| <span data-ttu-id="77820-155">La [perdita logaritmica](http://wiki.fast.ai/index.php/Log_Loss) misura le prestazioni di un modello di classificazione in cui l'input della stima è un valore di probabilità compreso tra 0,00 e 1,00.</span><span class="sxs-lookup"><span data-stu-id="77820-155">[Logarithmic loss](http://wiki.fast.ai/index.php/Log_Loss) measures the performance of a classification model where the prediction input is a probability value between 0.00 and 1.00.</span></span> <span data-ttu-id="77820-156">Questa metrica aumenta quando la probabilità stimata devia dall'etichetta effettiva.</span><span class="sxs-lookup"><span data-stu-id="77820-156">Log-loss increases as the predicted probability diverges from the actual label.</span></span> | <span data-ttu-id="77820-157">**Quanto più vicino a 0,00, tanto meglio**.</span><span class="sxs-lookup"><span data-stu-id="77820-157">**The closer to 0.00, the better**.</span></span> <span data-ttu-id="77820-158">In un modello perfetto, log-loss sarebbe uguale a 0,00.</span><span class="sxs-lookup"><span data-stu-id="77820-158">A perfect model would have a log-loss of 0.00.</span></span> <span data-ttu-id="77820-159">L'obiettivo dei modelli di Machine Learning è ridurre al minimo questo valore.</span><span class="sxs-lookup"><span data-stu-id="77820-159">The goal of our machine learning models is to minimize this value.</span></span>|
| <span data-ttu-id="77820-160">**Log-Loss Reduction**</span><span class="sxs-lookup"><span data-stu-id="77820-160">**Log-Loss Reduction**</span></span> | <span data-ttu-id="77820-161">La [riduzione della perdita logaritmica](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.multiclassclassificationmetrics.loglossreduction?view=ml-dotnet) può essere interpretata come un vantaggio del classificatore rispetto alla stima casuale.</span><span class="sxs-lookup"><span data-stu-id="77820-161">[Logarithmic loss reduction](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.data.multiclassclassificationmetrics.loglossreduction?view=ml-dotnet) can be interpreted as the advantage of the classifier over a random prediction.</span></span>| <span data-ttu-id="77820-162">**Il valore è compreso nell'intervallo da -inf a 1,00, dove 1,00 corrisponde a stime perfette e 0,00 indica stime medie**.</span><span class="sxs-lookup"><span data-stu-id="77820-162">**Ranges from -inf and 1.00, where 1.00 is perfect predictions and 0.00 indicates mean predictions**.</span></span> <span data-ttu-id="77820-163">Ad esempio, se il valore equivale a 0,20, può essere interpretato come "la probabilità di una stima corretta è il 20% maggiore rispetto alla stima casuale"</span><span class="sxs-lookup"><span data-stu-id="77820-163">For example, if the value equals 0.20, it can be interpreted as "the probability of a correct prediction is 20% better than random guessing"</span></span>|

<span data-ttu-id="77820-164">La micro-accuratezza è generalmente più indicata per le esigenze aziendali di stime di ML.</span><span class="sxs-lookup"><span data-stu-id="77820-164">Micro-accuracy is generally better aligned with the business needs of ML predictions.</span></span> <span data-ttu-id="77820-165">Se si vuole selezionare una singola metrica per scegliere la qualità dell'attività di classificazione multiclasse, è in genere preferibile puntare alla micro-accuratezza.</span><span class="sxs-lookup"><span data-stu-id="77820-165">If you want to select a single metric for choosing the quality of a multiclass classification task, it should usually be micro-accuracy.</span></span>

<span data-ttu-id="77820-166">Ad esempio, per un'attività di classificazione dei ticket di supporto: (mapping dei ticket in arrivo con i team di supporto)</span><span class="sxs-lookup"><span data-stu-id="77820-166">Example, for a support ticket classification task: (maps incoming tickets to support teams)</span></span>

- <span data-ttu-id="77820-167">Micro-accuratezza: con quale frequenza un ticket in ingresso viene classificato per il team corretto?</span><span class="sxs-lookup"><span data-stu-id="77820-167">Micro-accuracy -- how often does an incoming ticket get classified to the right team?</span></span>
- <span data-ttu-id="77820-168">Macro-accuratezza: con quale frequenza un ticket in ingresso è corretto per un tipico team?</span><span class="sxs-lookup"><span data-stu-id="77820-168">Macro-accuracy -- for an average team, how often is an incoming ticket correct for their team?</span></span>

<span data-ttu-id="77820-169">In questo esempio la macro-accuratezza assegna un peso eccessivo ai piccoli team. Un piccolo team che riceve solo 10 ticket all'anno viene conteggiato allo stesso modo di un grande team con 10.000 ticket all'anno.</span><span class="sxs-lookup"><span data-stu-id="77820-169">Macro-accuracy overweights small teams in this example; a small team which gets only 10 tickets per year counts as much as a large team with 10k tickets per year.</span></span> <span data-ttu-id="77820-170">La micro-accuratezza in questo caso si adatta meglio all'esigenza aziendale di calcolare la quantità di tempo/denaro che è possibile risparmiare automatizzando il processo di instradamento dei ticket.</span><span class="sxs-lookup"><span data-stu-id="77820-170">Micro-accuracy in this case correlates better with the business need of, "how much time/money can the company save by automating my ticket routing process".</span></span>

<span data-ttu-id="77820-171">Per altre informazioni sulle metriche di classificazione multiclasse, vedere gli articoli seguenti:</span><span class="sxs-lookup"><span data-stu-id="77820-171">For further details on multi-class classification metrics read the following articles:</span></span>

- [<span data-ttu-id="77820-172">Micro- and Macro-average of Precision, Recall and F-Score</span><span class="sxs-lookup"><span data-stu-id="77820-172">Micro- and Macro-average of Precision, Recall and F-Score</span></span>](http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html)
- [<span data-ttu-id="77820-173">Multiclass Classification with Imbalanced Dataset</span><span class="sxs-lookup"><span data-stu-id="77820-173">Multiclass Classification with Imbalanced Dataset</span></span>](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a)

## <a name="metrics-for-regression"></a><span data-ttu-id="77820-174">Metriche per la regressione</span><span class="sxs-lookup"><span data-stu-id="77820-174">Metrics for Regression</span></span>

| <span data-ttu-id="77820-175">Metrica</span><span class="sxs-lookup"><span data-stu-id="77820-175">Metrics</span></span>   |      <span data-ttu-id="77820-176">Descrizione</span><span class="sxs-lookup"><span data-stu-id="77820-176">Description</span></span>      |  <span data-ttu-id="77820-177">Obiettivo</span><span class="sxs-lookup"><span data-stu-id="77820-177">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="77820-178">**R-Squared**</span><span class="sxs-lookup"><span data-stu-id="77820-178">**R-Squared**</span></span> |  <span data-ttu-id="77820-179">[R-squared (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination) o *coefficiente di determinazione* rappresenta la potenza predittiva del modello come valore compreso tra -inf e 1,00.</span><span class="sxs-lookup"><span data-stu-id="77820-179">[R-squared (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), or *Coefficient of determination* represents the predictive power of the model as a value between -inf and 1.00.</span></span> <span data-ttu-id="77820-180">1,00 significa corrispondenza perfetta e la corrispondenza può essere arbitrariamente insufficiente, quindi i punteggi possono essere negativi.</span><span class="sxs-lookup"><span data-stu-id="77820-180">1.00 means there is a perfect fit, and the fit can be arbitrarly poor so the scores can be negative.</span></span> <span data-ttu-id="77820-181">Il punteggio 0,00 significa che il modello indovina il valore previsto per l'etichetta.</span><span class="sxs-lookup"><span data-stu-id="77820-181">A score of 0.00 means the model is guessing the expected value for the label.</span></span> <span data-ttu-id="77820-182">R2 misura il grado di prossimità dei valori dei dati di test effettivi ai valori stimati.</span><span class="sxs-lookup"><span data-stu-id="77820-182">R2 measures how close the actual test data values are to the predicted values.</span></span> | <span data-ttu-id="77820-183">**Quanto più vicino a 1,00, tanto migliore è la qualità**.</span><span class="sxs-lookup"><span data-stu-id="77820-183">**The closer to 1.00, the better quality**.</span></span> <span data-ttu-id="77820-184">Tuttavia, a volte i valori di R-squared bassi (ad esempio 0,50) possono essere perfettamente normali o sufficientemente validi per uno specifico scenario, mentre quelli alti non sono sempre validi e possono essere sospetti.</span><span class="sxs-lookup"><span data-stu-id="77820-184">However, sometimes low R-squared values (such as 0.50) can be entirely normal or good enough for your scenario and high R-squared values are not always good and be suspicious.</span></span> |
| <span data-ttu-id="77820-185">**Absolute-loss**</span><span class="sxs-lookup"><span data-stu-id="77820-185">**Absolute-loss**</span></span> |  <span data-ttu-id="77820-186">[Absolute-loss](https://en.wikipedia.org/wiki/Mean_absolute_error) o *errore assoluto medio* misura la prossimità delle stime ai risultati effettivi.</span><span class="sxs-lookup"><span data-stu-id="77820-186">[Absolute-loss](https://en.wikipedia.org/wiki/Mean_absolute_error) or *Mean absolute error (MAE)* measures how close the predictions are to the actual outcomes.</span></span> <span data-ttu-id="77820-187">Corrisponde alla media di tutti gli errori del modello, dove un errore del modello è la distanza tra il valore di etichetta stimato e quello corretto.</span><span class="sxs-lookup"><span data-stu-id="77820-187">It is the average of all the model errors, where model error is the absolute distance between the predicted label value and the correct label value.</span></span> <span data-ttu-id="77820-188">Questo errore di stima viene calcolato per ogni record del set di dati di test.</span><span class="sxs-lookup"><span data-stu-id="77820-188">This prediction error is calculated for each record of the test data set.</span></span> <span data-ttu-id="77820-189">Infine, viene calcolato il valore medio per tutti gli errori assoluti registrati.</span><span class="sxs-lookup"><span data-stu-id="77820-189">Finally, the mean value is calculated for all recorded absolute errors.</span></span>| <span data-ttu-id="77820-190">**Quanto più vicino a 0,00, tanto migliore è la qualità**.</span><span class="sxs-lookup"><span data-stu-id="77820-190">**The closer to 0.00, the better quality.**</span></span> <span data-ttu-id="77820-191">Si noti che l'errore assoluto medio usa la stessa scala dei dati misurati, ossia non è normalizzato in base a un intervallo specifico.</span><span class="sxs-lookup"><span data-stu-id="77820-191">Note that the mean absolute error uses the same scale as the data being measured (is not normalized to specific range).</span></span> <span data-ttu-id="77820-192">Le metriche Absolute-loss, Squared-loss e RMS-loss possono essere usate solo per eseguire confronti tra modelli per lo stesso set di dati o per un set di dati con una distribuzione simile dei valori di etichetta.</span><span class="sxs-lookup"><span data-stu-id="77820-192">Absolute-loss, Squared-loss, and RMS-loss can only be used to make comparisons between models for the same dataset or dataset with a smilar label value distribution.</span></span> |
| <span data-ttu-id="77820-193">**Squared-loss**</span><span class="sxs-lookup"><span data-stu-id="77820-193">**Squared-loss**</span></span> |  <span data-ttu-id="77820-194">[Squared-loss](https://en.wikipedia.org/wiki/Mean_squared_error) o *errore quadratico medio*, anche detto *deviazione quadratica media*, indica la prossimità di una linea di regressione a un set di valori di dati di test.</span><span class="sxs-lookup"><span data-stu-id="77820-194">[Squared-loss](https://en.wikipedia.org/wiki/Mean_squared_error) or *Mean Squared Error (MSE)*, also called or *Mean Squared Deviation (MSD)* tells you how close a regression line is to a set of test data values.</span></span> <span data-ttu-id="77820-195">A questo scopo esegue la quadratura delle distanze tra i punti e la linea di regressione (queste distanze sono gli "errori").</span><span class="sxs-lookup"><span data-stu-id="77820-195">It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them.</span></span> <span data-ttu-id="77820-196">La quadratura assegna più peso alle differenze maggiori.</span><span class="sxs-lookup"><span data-stu-id="77820-196">The squaring gives more weight to larger differences.</span></span> | <span data-ttu-id="77820-197">È sempre un valore non negativo e i **valori migliori sono quelli più vicini a 0,00**.</span><span class="sxs-lookup"><span data-stu-id="77820-197">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="77820-198">A seconda dei dati, può essere impossibile ottenere un valore molto piccolo per l'errore quadratico medio.</span><span class="sxs-lookup"><span data-stu-id="77820-198">Depending on your data, it may be impossible to get a very small value for the mean squared error.</span></span>|
| <span data-ttu-id="77820-199">**RMS-loss**</span><span class="sxs-lookup"><span data-stu-id="77820-199">**RMS-loss**</span></span> |  <span data-ttu-id="77820-200">[RMS-loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) o *radice dell'errore quadratico medio*, anche detto *radice della deviazione quadratica media*, misura la differenza tra i valori stimati da un modello e i valori effettivamente osservati nell'ambiente del modello.</span><span class="sxs-lookup"><span data-stu-id="77820-200">[RMS-loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) or *Root Mean Squared Error (RMSE)* (also called *Root Mean Square Deviation, RMSD*), measures the difference between values predicted by a model and the values actually observed from the environment that is being modeled.</span></span> <span data-ttu-id="77820-201">RMS-loss è la radice quadrata di Squared-loss e ha la stessa unità come etichetta, simile a absolute-loss ma assegnando più peso alle differenze maggiori.</span><span class="sxs-lookup"><span data-stu-id="77820-201">RMS-loss is the square root of Squared-loss and has the same units as the label, similar to the abolute-loss though giving more weight to larger diferences.</span></span> <span data-ttu-id="77820-202">La radice dell'errore quadratico medio viene comunemente usata in climatologia, previsioni e analisi di regressione per verificare i risultati sperimentali.</span><span class="sxs-lookup"><span data-stu-id="77820-202">Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.</span></span> | <span data-ttu-id="77820-203">È sempre un valore non negativo e i **valori migliori sono quelli più vicini a 0,00**.</span><span class="sxs-lookup"><span data-stu-id="77820-203">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="77820-204">RMSD è una misura dell'accuratezza, per confrontare gli errori di previsione di diversi modelli per uno specifico set di dati e non tra set di dati, in quando è dipendente dalla scala.</span><span class="sxs-lookup"><span data-stu-id="77820-204">RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.</span></span>|

<span data-ttu-id="77820-205">Per altre informazioni sulle metriche di regressione, vedere gli articoli seguenti:</span><span class="sxs-lookup"><span data-stu-id="77820-205">For further details on regression metrics, read the following articles:</span></span>

- [<span data-ttu-id="77820-206">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</span><span class="sxs-lookup"><span data-stu-id="77820-206">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</span></span>](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)
- [<span data-ttu-id="77820-207">How To Interpret R-squared in Regression Analysis</span><span class="sxs-lookup"><span data-stu-id="77820-207">How To Interpret R-squared in Regression Analysis</span></span>](https://statisticsbyjim.com/regression/interpret-r-squared-regression)
- [<span data-ttu-id="77820-208">R-Squared Definition</span><span class="sxs-lookup"><span data-stu-id="77820-208">R-Squared Definition</span></span>](https://www.investopedia.com/terms/r/r-squared.asp)
- [<span data-ttu-id="77820-209">Mean Squared Error Definition</span><span class="sxs-lookup"><span data-stu-id="77820-209">Mean Squared Error Definition</span></span>](https://www.statisticshowto.datasciencecentral.com/mean-squared-error/)
- [<span data-ttu-id="77820-210">What are Mean Squared Error and Root Mean Squared Error?</span><span class="sxs-lookup"><span data-stu-id="77820-210">What are Mean Squared Error and Root Mean Squared Error?</span></span>](https://www.vernier.com/til/1014/)
